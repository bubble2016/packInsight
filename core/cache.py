# -*- coding: utf-8 -*-
"""
ç£ç›˜æŒä¹…åŒ–ç¼“å­˜ç³»ç»Ÿ
"""
import os
import time
import hashlib
import pickle
import json

from config import CACHE_MAX_AGE_DAYS
from .logger import print_log


class DataCache:
    """æ•°æ®ç¼“å­˜ç®¡ç†å™¨ - ç£ç›˜æŒä¹…åŒ–ï¼Œé¿å…é‡å¤è®¡ç®—"""
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._init_cache()
        return cls._instance
    
    def _init_cache(self):
        """åˆå§‹åŒ–ç¼“å­˜ç›®å½•å’Œç´¢å¼•"""
        # ç¼“å­˜ç›®å½•æ”¾åœ¨ç”¨æˆ·æ–‡æ¡£ç›®å½•ä¸‹
        self.cache_dir = os.path.join(os.path.expanduser('~'), '.packing_station_cache')
        self.index_file = os.path.join(self.cache_dir, 'cache_index.json')
        
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
        
        # åŠ è½½ç¼“å­˜ç´¢å¼•
        self.index = self._load_index()
        
        # è‡ªåŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜
        self._cleanup_old_cache()
    
    def _load_index(self):
        """åŠ è½½ç¼“å­˜ç´¢å¼•"""
        if os.path.exists(self.index_file):
            try:
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return {}
        return {}
    
    def _save_index(self):
        """ä¿å­˜ç¼“å­˜ç´¢å¼•"""
        try:
            with open(self.index_file, 'w', encoding='utf-8') as f:
                json.dump(self.index, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"\033[1;33m[CACHE] æ— æ³•ä¿å­˜ç¼“å­˜ç´¢å¼•: {e}\033[0m")
    
    def _cleanup_old_cache(self, max_age_days=None):
        """æ¸…ç†è¶…è¿‡æŒ‡å®šå¤©æ•°çš„æ—§ç¼“å­˜"""
        if max_age_days is None:
            max_age_days = CACHE_MAX_AGE_DAYS
        
        now = time.time()
        max_age_seconds = max_age_days * 24 * 3600
        keys_to_remove = []
        
        for key, info in self.index.items():
            cache_time = info.get('created', 0)
            if now - cache_time > max_age_seconds:
                keys_to_remove.append(key)
                cache_file = os.path.join(self.cache_dir, f"{key}.pkl")
                if os.path.exists(cache_file):
                    try:
                        os.remove(cache_file)
                    except Exception:
                        pass
        
        for key in keys_to_remove:
            del self.index[key]
        
        if keys_to_remove:
            self._save_index()
            print_log(f"å·²æ¸…ç† {len(keys_to_remove)} ä¸ªè¿‡æœŸç¼“å­˜", "CACHE")
    
    def _get_cache_key(self, file_path, sheet_names):
        """ç”Ÿæˆç¼“å­˜é”®ï¼šåŸºäºæ–‡ä»¶è·¯å¾„ + ä¿®æ”¹æ—¶é—´ + é€‰ä¸­çš„å·¥ä½œè¡¨"""
        try:
            mtime = os.path.getmtime(file_path)
            key_str = f"{file_path}:{mtime}:{','.join(sorted(sheet_names))}"
            return hashlib.md5(key_str.encode()).hexdigest()
        except Exception:
            return None
    
    def get(self, file_path, sheet_names, key_name):
        """è·å–ç¼“å­˜æ•°æ®"""
        cache_key = self._get_cache_key(file_path, sheet_names)
        if not cache_key:
            return None
        
        full_key = f"{cache_key}_{key_name}"
        
        # æ£€æŸ¥ç´¢å¼•ä¸­æ˜¯å¦å­˜åœ¨
        if full_key not in self.index:
            return None
        
        # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ˜¯å¦åŒ¹é…
        cached_mtime = self.index[full_key].get('file_mtime', 0)
        try:
            current_mtime = os.path.getmtime(file_path)
            if abs(cached_mtime - current_mtime) > 1:  # å…è®¸1ç§’è¯¯å·®
                return None
        except Exception:
            return None
        
        # è¯»å–ç¼“å­˜æ–‡ä»¶
        cache_file = os.path.join(self.cache_dir, f"{full_key}.pkl")
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception:
                return None
        return None
    
    def set(self, file_path, sheet_names, key_name, value):
        """è®¾ç½®ç¼“å­˜æ•°æ®ï¼ˆæŒä¹…åŒ–åˆ°ç£ç›˜ï¼‰"""
        cache_key = self._get_cache_key(file_path, sheet_names)
        if not cache_key:
            return
        
        full_key = f"{cache_key}_{key_name}"
        cache_file = os.path.join(self.cache_dir, f"{full_key}.pkl")
        
        try:
            # å†™å…¥ç¼“å­˜æ–‡ä»¶
            with open(cache_file, 'wb') as f:
                pickle.dump(value, f)
            
            # æ›´æ–°ç´¢å¼•
            self.index[full_key] = {
                'created': time.time(),
                'file_path': file_path,
                'file_mtime': os.path.getmtime(file_path),
                'sheets': sheet_names,
                'key_name': key_name
            }
            self._save_index()
            print_log(f"ğŸ’¾ å·²ç¼“å­˜: {key_name}", "CACHE")
        except Exception as e:
            print(f"\033[1;33m[CACHE] ç¼“å­˜å†™å…¥å¤±è´¥: {e}\033[0m")
    
    def is_valid(self, file_path, sheet_names):
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆæ–‡ä»¶æœªè¢«ä¿®æ”¹ï¼‰"""
        return self.get(file_path, sheet_names, 'df') is not None


# åˆå§‹åŒ–å…¨å±€ç¼“å­˜ç®¡ç†å™¨
data_cache = DataCache()
